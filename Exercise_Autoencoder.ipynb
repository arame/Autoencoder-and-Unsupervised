{"cells":[{"cell_type":"markdown","metadata":{"id":"xPQM73S_UQLk"},"source":["# Autoencoders\n","Autoencoders are a class of neural network that attempt to recreate the input\n","as their target using back-propagation. An autoencoder consists of two parts; an **encoder** and a **decoder**. The encoder will read the input and compress it to a compact representation, and the decoder will read the compact representation and recreate the input from it. In other words, the autoencoder tries to learn the identity function by minimizing the reconstruction error. They have an inherent capability to learn\n","a compact representation of data. They are at the center of deep belief networks\n","and find applications in image reconstruction, clustering, machine translation,\n","and much more.\n","\n","This exercise aims to test your understanding of autoencoder architecture, and how it can be used to denoise an image. We will build a convolutional autoencoder. Combining your knowledge of a Vanilla/Denoising Autoencoder and Convolutional Networks.\n","\n","The notebook has five Exercises followed by an optional exercise."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"jwqKr35LhRnl"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","C:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","C:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","C:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","C:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","C:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"]}],"source":["#@title Import Modules \n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow.keras as K\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D\n","\n","\n","np.random.seed(42)\n","tf.random.set_random_seed(42)"]},{"cell_type":"markdown","metadata":{"id":"EIgJlvDIM7Zw"},"source":["## AutoEncoder  Architecture\n","The number of hidden units in the autoencoder is typically less than the number of input (and output) units. This forces the encoder to learn a compressed representation of the input, which the decoder reconstructs. If there is a structure in the input data in the form of correlations between input features, then the autoencoder will discover some of these correlations, and end up learning a low-dimensional representation of the data similar to that learned using principal component analysis (PCA).\n","\n","Once trained\n","* We can discard **decoder** and use **Encoder** to optain a compact representation of input.\n","* We can cascade Encoder to a classifier.\n","\n","The encoder and decoder components of an autoencoder can be implemented using either dense, convolutional, or recurrent networks, depending on the kind of data that is being modeled.\n","\n","Below we define an encoder and a decoder using Convolutional layers. Both consist of three convolutional layers. Each layer in Encoder has a corresponding layer in decoder, thus in this case it is like three autoencoders stacked over each other. This is also called **Stacked Autoencoders** \n","\n","![](https://drive.google.com/uc?id=1UzM67qf1VE_8akrCgiohKjUIHoO_2x4E)\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"cellView":"form","id":"71xVjHhTE5Z8"},"outputs":[],"source":["#@title Encoder\n","class Encoder(K.layers.Layer):\n","    def __init__(self, filters):\n","        super(Encoder, self).__init__()\n","        self.conv1 = Conv2D(filters=filters[0], kernel_size=3, strides=1, activation='relu', padding='same')\n","        self.conv2 = Conv2D(filters=filters[1], kernel_size=3, strides=1, activation='relu', padding='same')\n","        self.conv3 = Conv2D(filters=filters[2], kernel_size=3, strides=1, activation='relu', padding='same')\n","        self.pool = MaxPooling2D((2, 2), padding='same')\n","               \n","    \n","    def call(self, input_features):\n","        print(\"Encoder call\")\n","        x = self.conv1(input_features)\n","        print(\"Ex1\", x.shape)\n","        x = self.pool(x)\n","        print(\"Ex2\", x.shape)\n","        x = self.conv2(x)\n","        x = self.pool(x)\n","        x = self.conv3(x)\n","        x = self.pool(x)\n","        return x\n","        "]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","id":"prFVAaj_FM9L"},"outputs":[],"source":["#@title Decoder\n","class Decoder(K.layers.Layer):\n","    def __init__(self, filters):\n","        super(Decoder, self).__init__()\n","        self.conv1 = Conv2D(filters=filters[2], kernel_size=3, strides=1, activation='relu', padding='same')\n","        self.conv2 = Conv2D(filters=filters[1], kernel_size=3, strides=1, activation='relu', padding='same')\n","        self.conv3 = Conv2D(filters=filters[0], kernel_size=3, strides=1, activation='relu', padding='valid')\n","        self.conv4 = Conv2D(1, 3, 1, activation='sigmoid', padding='same')\n","        self.upsample = UpSampling2D((2, 2))\n","  \n","    def call(self, encoded):\n","        x = self.conv1(encoded)\n","        #print(\"dx1\", x.shape)\n","        x = self.upsample(x)\n","        #print(\"dx2\", x.shape)\n","        x = self.conv2(x)\n","        x = self.upsample(x)\n","        x = self.conv3(x)\n","        x = self.upsample(x)\n","        return self.conv4(x)"]},{"cell_type":"markdown","metadata":{"id":"5xttby3Lw9pT"},"source":["## Denoising Autoencoder\n","\n","When we train the autoencoder, we can train it directly on the raw images or we can add noise to the input images while training. When the autoencoder is trained on noisy data, it gets an even interesting property--it can reconstruct noisy images. In other words--you give it an image with noise and it will remove the noise from it."]},{"cell_type":"markdown","metadata":{"id":"-Y4h5e6CTvrN"},"source":["## Exercise 1:\n","In this exercise we will train the stacked autoencoder in four steps:\n","* In [Step 1](#step1) choose the noise = 0\n","* Complete the [Step 2](#step2)\n","* In the [Step 3](#step3) choose filters as [16, 32, 64] for Encoder and [64, 32, 16] for Decoder.\n","* Perform [Step 4](#step4) for batch size of 64 and 10 epochs\n","* Reflect on the plotted images what do you see?"]},{"cell_type":"markdown","metadata":{"id":"zIFHypkQ2GnK"},"source":["**Answer 1** (Double click to edit)*italicized text*"]},{"cell_type":"markdown","metadata":{"id":"Qz9itTNs0FBG"},"source":["<a id='step1'></a>\n","### Step 1:\n","Read the dataset, process it for noise = 0"]},{"cell_type":"code","execution_count":5,"metadata":{"cellView":"form","id":"MGQwAOyKEi1I"},"outputs":[],"source":["#@title Dataset Reading and Processing\n","Noise = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n","(x_train, _), (x_test, _) = K.datasets.mnist.load_data()\n","\n","x_train = x_train / 255.\n","x_test = x_test / 255.\n","\n","x_train = np.reshape(x_train, (len(x_train),28, 28, 1))\n","x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n","\n","noise = Noise\n","x_train_noisy = x_train + noise * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n","x_test_noisy = x_test + noise * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n","\n","x_train_noisy = np.clip(x_train_noisy, 0, 1)\n","x_test_noisy = np.clip(x_test_noisy, 0, 1)\n","\n","x_train_noisy = x_train_noisy.astype('float32')\n","x_test_noisy = x_test_noisy.astype('float32')"]},{"cell_type":"markdown","metadata":{"id":"2P8m_M3xFfl-"},"source":["<a id='another_cell'></a>\n","### Step 2\n","\n","You need to complete the code below. We will be using the Encoder and Decoder architectures that we have defined above to build an autoencoder. In the code below replace `...` with right code. "]},{"cell_type":"code","execution_count":6,"metadata":{"id":"TfAUWna6FdNz"},"outputs":[],"source":["class Autoencoder(K.Model):\n","    def __init__(self, filters_encoder, filters_decoder):\n","        super(Autoencoder, self).__init__()\n","        self.loss = []\n","        self.encoder = Encoder(filters_encoder)\n","        self.decoder = Decoder(filters_decoder)\n","\n","    def call(self, input_features):\n","        print(f\"Input features shape: {input_features.shape}\")\n","        encoded = self.encoder(input_features)\n","        print(f\"Input features shape: {encoded.shape}\")\n","        reconstructed = self.decoder(encoded)\n","        print(f\"Reconstructed shape {reconstructed.shape}\")\n","        return reconstructed\n"]},{"cell_type":"markdown","metadata":{"id":"4S7xGoIN2ZO_"},"source":["## Exercise 2:\n","In this exercise we will make only one change, in step 3 choose filters as: `[16, 32, 64]` for both Encoder and Decoder.\n"," Try training the Autoencoder. What happens? Why do you think it is so?"]},{"cell_type":"markdown","metadata":{"id":"dElvF1I45wJt"},"source":["**Answer 2** (Double click to edit)"]},{"cell_type":"markdown","metadata":{"id":"fj2d1Qqs7_eK"},"source":["## Exercise 3:\n","\n","Now we will introduce noise of 0.2 in the training dataset. Train an autoencoder with filters [64,32,16] for encoder and [16,32,64] for decoder and observe the reconstrucred images.\n","\n","\n","What do you find? Is the autoencoder able to recognize noisy digits?\n"]},{"cell_type":"markdown","metadata":{"id":"dNvlTwLK877U"},"source":["**Answer 3** (Double click to edit)"]},{"cell_type":"markdown","metadata":{"id":"BiGMz4Kd9AZk"},"source":["## Exercise 4:\n","\n","Let us be more adventurous with the same Encoder-Decoder architecture, we increase the noise and observe the reconstrucred images.\n","\n","\n","What do you find? Till what noise value is the autoencoder able to reconstruct images? Till what noise level you (human) can recognize the digits in the noisy image.\n"]},{"cell_type":"markdown","metadata":{"id":"IL5Aps7f9io1"},"source":["**Answer 4** (Double click to edit)"]},{"cell_type":"markdown","metadata":{"id":"pVTaR4St4PvR"},"source":["<a id='step3'></a>"]},{"cell_type":"markdown","metadata":{"id":"BGeWEEixZBhG"},"source":["### Step 3:\n","\n","We have built Convolutional Autoencoder. That is both Encoder and Decoder are buit using Convolutional layers. Below you need to select "]},{"cell_type":"code","execution_count":7,"metadata":{"cellView":"form","id":"QC8Bx7u0n_OP"},"outputs":[],"source":["#@title Select Filters for Encoder & Decoder\n","filter_encoder_0 = 16 #@param {type:\"slider\", min:8, max:256, step:2}\n","filter_encoder_1 = 32 #@param {type:\"slider\", min:8, max:256, step:2}\n","filter_encoder_2 = 64 #@param {type:\"slider\", min:8, max:256, step:2}\n","\n","filters_en = [filter_encoder_0,filter_encoder_1,filter_encoder_2]\n","\n","\n","filter_decoder_0 = 64 #@param {type:\"slider\", min:8, max:256, step:2}\n","filter_decoder_1 = 32 #@param {type:\"slider\", min:8, max:256, step:2}\n","filter_decoder_2 = 16 #@param {type:\"slider\", min:8, max:256, step:2}\n","\n","filters_de = [filter_decoder_0,filter_decoder_1,filter_decoder_2]\n","\n","\n","model = Autoencoder(filters_en, filters_de)\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam')\n"]},{"cell_type":"markdown","metadata":{"id":"zfpau9KBuCS3"},"source":["### Step 4:\n","Choose the appropriate batch_size and epochs"]},{"cell_type":"code","execution_count":8,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":266},"id":"U7yqEyKqJ595","outputId":"d0fe28b4-a3fe-4766-97c3-2b73c8812810"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input features shape: (?, 28, 28, 1)\n","Input features shape: (?, 4, 4, 64)\n","Reconstructed shape (?, 28, 28, 1)\n","Train on 60000 samples, validate on 10000 samples\n","Epoch 1/10\n","60000/60000 [==============================] - 87s 1ms/step - loss: 44.2619 - val_loss: 44.2218\n","Epoch 2/10\n","60000/60000 [==============================] - 92s 2ms/step - loss: 44.2619 - val_loss: 44.2218\n","Epoch 3/10\n","60000/60000 [==============================] - 94s 2ms/step - loss: 44.2619 - val_loss: 44.2218\n","Epoch 4/10\n","60000/60000 [==============================] - 105s 2ms/step - loss: 44.2619 - val_loss: 44.2218\n","Epoch 5/10\n","60000/60000 [==============================] - 95s 2ms/step - loss: 44.2619 - val_loss: 44.2218\n","Epoch 6/10\n","60000/60000 [==============================] - 102s 2ms/step - loss: 44.2619 - val_loss: 44.2218\n","Epoch 7/10\n","60000/60000 [==============================] - 97s 2ms/step - loss: 44.2619 - val_loss: 44.2218\n","Epoch 8/10\n","60000/60000 [==============================] - 88s 1ms/step - loss: 44.2619 - val_loss: 44.2218\n","Epoch 9/10\n","60000/60000 [==============================] - 91s 2ms/step - loss: 44.2619 - val_loss: 44.2218\n","Epoch 10/10\n","60000/60000 [==============================] - 88s 1ms/step - loss: 44.2619 - val_loss: 44.2218\n"]}],"source":["#@title Train the model\n","BATCH_SIZE = 64 #@param {type:\"slider\", min:32, max:2000, step:10}\n","EPOCHS = 10 #@param {type:\"slider\", min:1, max:100, step:1}\n","batch_size = BATCH_SIZE\n","max_epochs = EPOCHS\n","loss = model.fit(x_train_noisy,\n","                x_train,\n","                validation_data=(x_test_noisy, x_test),\n","                epochs=max_epochs,\n","                batch_size=batch_size)\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input features shape: (10000, 28, 28, 1)\n"]},{"ename":"AttributeError","evalue":"'tuple' object has no attribute 'ndims'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m<ipython-input-25-dc9d85891777>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#print(x_test_noisy.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_noisy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#print(model_x[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-6-45a625e88650>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_features)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Input features shape: {input_features.shape}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Input features shape: {encoded.shape}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mreconstructed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-3-9c72e23929c8>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_features)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;31m#print(\"Ex1\", x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    751\u001b[0m       \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mbuild_graph\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1461\u001b[0m           \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1462\u001b[0m           spec.max_ndim is not None):\n\u001b[1;32m-> 1463\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1464\u001b[0m           raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0;32m   1465\u001b[0m                            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ndims'"]}],"source":["#print(x_test_noisy.shape)\n","model_x = model(x_test_noisy)\n","#print(model_x[0])"]},{"cell_type":"code","execution_count":24,"metadata":{"cellView":"form","id":"L817CFR2LgNR"},"outputs":[{"name":"stdout","output_type":"stream","text":["index = 0\n","Input features shape: (10000, 28, 28, 1)\n"]},{"ename":"AttributeError","evalue":"'tuple' object has no attribute 'ndims'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m<ipython-input-24-934d6b3122ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# display reconstruction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_noisy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_xaxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_yaxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-6-45a625e88650>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_features)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Input features shape: {input_features.shape}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Input features shape: {encoded.shape}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mreconstructed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-3-9c72e23929c8>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_features)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;31m#print(\"Ex1\", x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    751\u001b[0m       \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mbuild_graph\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1461\u001b[0m           \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1462\u001b[0m           spec.max_ndim is not None):\n\u001b[1;32m-> 1463\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1464\u001b[0m           raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0;32m   1465\u001b[0m                            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ndims'"]},{"data":{"image/png":"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","text/plain":["<Figure size 1440x288 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["#@title Reconstructed images\n","number = 10  # how many digits we will display\n","plt.figure(figsize=(20, 4))\n","for index in range(number):\n","    print(f\"index = {index}\")\n","    # display original\n","    ax = plt.subplot(2, number, index + 1)\n","    plt.imshow(x_test_noisy[index].reshape(28, 28), cmap='gray')\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","\n","    # display reconstruction\n","    ax = plt.subplot(2, number, index + 1 + number)\n","    plt.imshow(tf.reshape(model(x_test_noisy)[index], (28, 28)), cmap='gray')\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zrFJlz5b94dI"},"source":["## Optional Exercise\n","Construct a Sparse Autoencoder with Dense layer/s, train it on noisy images as before. See how the hidden dimensions influence the reconstruction. Which is one is better for denoising, the convolution Encoder/Decoder or Dense Encoder/Decoder, why?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wz6WiM7t-Hjm"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"Exercise_Autoencoder.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":0}
